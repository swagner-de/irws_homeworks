{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRWS Homework 03\n",
    "*Sebastian Wagner*\n",
    "## Exercise 1\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining the documents here and set the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOKENIZER = TreebankWordTokenizer()\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "d1 = \"Frodo and Sam were trembling in the darkness, surrounded in darkness by hundreds of blood-thirsty orc. Sam was certain these beasts were about to taste the scent of their flesh.\"\n",
    "d2 = \"The faceless black beast then stabbed Frodo. He felt like every nerve in his body was hurting. Suddenly, he thought of Sam and his calming smile. Frodo had betrayed him.\"\n",
    "d3 = \"Frodo’s sword was radiating blue, stronger and stronger every second. Orc were getting closer. And these weren’t just regular orc either, Uruk-Hai were among them. Frodo had killed regular orc before, but he had never stabbed an Uruk-Hai, not with the blue stick.\"\n",
    "d4 = \"Sam was carrying a small lamp, shedding some blue light. He was afraid that orc might spot him, but it was the only way to avoid deadly pitfalls of Mordor.\"\n",
    "\n",
    "docs = [d1.lower(), d2.lower(), d3.lower(), d4.lower()]\n",
    "\n",
    "\n",
    "ORDERED_VOC = [t.lower() for t in [\"Frodo\", \"Sam\", \"beast\", \"orc\", \"blue\"]]\n",
    "VOC = set(ORDERED_VOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining some functions here that will be needed later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stolen from http://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def reduce_to_voc(tokens):\n",
    "    t_rel = []\n",
    "    for t in tokens:\n",
    "        if t in VOC:\n",
    "            t_rel.append(t)\n",
    "    return t_rel\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    for t in tagged:\n",
    "        try:\n",
    "            yield LEMMATIZER.lemmatize(t[0], get_wordnet_pos(t[1]))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "def tokenize(doc):\n",
    "    doc = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in doc]).split())\n",
    "    result = []\n",
    "    for token in TOKENIZER.tokenize(doc):\n",
    "        result.append(token.lower())\n",
    "    return result\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = tokenize(doc)\n",
    "    tokens = lemmatize(tokens)\n",
    "    tokens = reduce_to_voc(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do preprocessing, the follwoing tokens remain\n",
    "documents are numbered horizontally, commencing from doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['frodo', 'sam', 'orc', 'sam', 'beast'],\n",
      " ['beast', 'frodo', 'sam', 'frodo'],\n",
      " ['blue', 'orc', 'orc', 'frodo', 'orc', 'blue'],\n",
      " ['sam', 'blue', 'orc']]\n"
     ]
    }
   ],
   "source": [
    "docs_prep = []\n",
    "for doc in docs:\n",
    "    docs_prep.append(preprocess(doc))\n",
    "pprint(docs_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frodo', 'sam', 'beast', 'orc', 'blue']\n",
      "array([[ 0.75,  0.  ,  0.  ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.75,  0.  ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.5 ,  0.  ,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.  ,  0.75,  0.  ],\n",
      "       [ 0.  ,  0.  ,  0.  ,  0.  ,  0.5 ]])\n"
     ]
    }
   ],
   "source": [
    "def compute_idf(docs):\n",
    "    mat = np.zeros((len(ORDERED_VOC), len(ORDERED_VOC)))\n",
    "    no_docs = len(docs)\n",
    "    for doc in docs:\n",
    "        for i in range(len(ORDERED_VOC)):\n",
    "            term = ORDERED_VOC[i]\n",
    "            mat[i,i] += 1 if term in doc else 0\n",
    "    return np.divide(mat, no_docs)\n",
    "\n",
    "idf = compute_idf(docs_prep)\n",
    "pprint(ORDERED_VOC)\n",
    "pprint(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the tf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.,  2.,  1.,  1.,  0.],\n",
      "       [ 2.,  1.,  1.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  3.,  2.],\n",
      "       [ 0.,  1.,  0.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "def compute_tf(docs):\n",
    "    mat = np.zeros((len(docs) ,len(ORDERED_VOC)))\n",
    "    for d in range(len(docs)):\n",
    "        doc = docs[d]\n",
    "        for t in range(len(ORDERED_VOC)):\n",
    "            term = ORDERED_VOC[t]\n",
    "            mat[d][t] = doc.count(term)\n",
    "    return mat\n",
    "\n",
    "tf = compute_tf(docs_prep)\n",
    "pprint(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the tf-idf matrix, then transpose it so it is in document-term format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.75,  1.5 ,  0.75,  0.  ],\n",
      "       [ 1.5 ,  0.75,  0.  ,  0.75],\n",
      "       [ 0.5 ,  0.5 ,  0.  ,  0.  ],\n",
      "       [ 0.75,  0.  ,  2.25,  0.75],\n",
      "       [ 0.  ,  0.  ,  1.  ,  0.5 ]])\n"
     ]
    }
   ],
   "source": [
    "tf_idf = np.matmul(tf, idf)\n",
    "term_doc_tfidf = np.transpose(tf_idf)\n",
    "pprint(term_doc_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.46176305, -0.4352475 , -0.73434277,  0.08708796, -0.22470177],\n",
      "       [-0.40142314, -0.5883274 ,  0.66623669,  0.16253806, -0.14980118],\n",
      "       [-0.13572251, -0.28374531, -0.05707485, -0.29992719,  0.89880706],\n",
      "       [-0.72507269,  0.52902442,  0.11562721, -0.41883529, -0.07490059],\n",
      "       [-0.28545288,  0.3225713 , -0.01556212,  0.83702912,  0.33705265]]),\n",
      " array([[ 3.10777963,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  1.96446704,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  1.04054244,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.29554427]]),\n",
      " array([[-0.50200559, -0.34158573, -0.72823333, -0.31778261],\n",
      "       [-0.48564371, -0.62917291,  0.60395038,  0.05946061],\n",
      "       [ 0.48703533, -0.60581294, -0.29422921,  0.55607233],\n",
      "       [-0.5243445 ,  0.34706097, -0.13545965,  0.76567762]])]\n"
     ]
    }
   ],
   "source": [
    "def svd(mat):\n",
    "    return np.linalg.svd(mat)\n",
    "    \n",
    "u, sigma, v = svd(term_doc_tfidf)\n",
    "sigma = np.diag(sigma)\n",
    "    \n",
    "pprint([u, sigma, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### c)\n",
    "# unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 3.10777963,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  1.96446704,  0.        ,  0.        ]])\n",
      "array([[-1.56012276, -1.50927363,  1.51359849, -1.62954716],\n",
      "       [-0.67103391, -1.23598945, -1.19009955,  0.68178984]])\n"
     ]
    }
   ],
   "source": [
    "sigma_2 = sigma[:2][:]\n",
    "pprint(sigma_2)\n",
    "\n",
    "dense_vec = np.matmul(sigma_2, np.transpose(v))\n",
    "pprint(dense_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "# unfinished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.  ],\n",
      "       [ 0.75],\n",
      "       [ 0.  ],\n",
      "       [ 0.75],\n",
      "       [ 0.5 ]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (5,2) and (5,1) not aligned: 2 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-e399b82077f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mu_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdense_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (5,2) and (5,1) not aligned: 2 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "query= ['sam', 'blue', 'orc']\n",
    "tf_query = compute_tf([query])\n",
    "tf_idf_query = np.transpose(np.matmul(tf_query, idf))\n",
    "pprint(tf_idf_query)\n",
    "\n",
    "u_2 = u[:][:2]\n",
    "       \n",
    "dense_query = np.matmul(np.transpose(u_2), tf_idf_query)\n",
    "pprint(dense_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17, 0.21, 0.35, 0.44, 0.49, 0.39, 0.09, 0.07, 0.37, 0.24]]\n",
      "Cluster 0\n",
      "\tNone\n",
      "[[0.49, 0.48, 0.44, 0.09, 0.24, 0.2, 0.41, 0.16, 0.1, 0.15]]\n",
      "Cluster 1\n",
      "\tNone\n",
      "[[0.41, 0.36, 0.27, 0.19, 0.15, 0.42, 0.23, 0.42, 0.02, 0.42]]\n",
      "Cluster 2\n",
      "\tNone\n",
      "[[0.31, 0.41, 0.21, 0.19, 0.47, 0.28, 0.21, 0.39, 0.16, 0.38]]\n",
      "Cluster 3\n",
      "\tNone\n",
      "[[0.46, 0.12, 0.21, 0.25, 0.38, 0.38, 0.46, 0.23, 0.31, 0.14]]\n",
      "Cluster 4\n",
      "\tNone\n",
      "[[0.13, 0.33, 0.28, 0.42, 0.07, 0.13, 0.58, 0.15, 0.0, 0.49]]\n",
      "Cluster 5\n",
      "\tNone\n",
      "[[0.21, 0.09, 0.07, 0.09, 0.3, 0.54, 0.24, 0.43, 0.51, 0.21]]\n",
      "Cluster 6\n",
      "\tNone\n",
      "[[0.18, 0.39, 0.42, 0.05, 0.41, 0.1, 0.52, 0.12, 0.14, 0.38]]\n",
      "Cluster 7\n",
      "\tNone\n",
      "[[0.4, 0.51, 0.01, 0.1, 0.12, 0.22, 0.26, 0.34, 0.42, 0.38]]\n",
      "Cluster 8\n",
      "\tNone\n"
     ]
    }
   ],
   "source": [
    "doc_term_idfvector = [\n",
    "    [0.17, 0.21, 0.35, 0.44, 0.49, 0.39, 0.09, 0.07, 0.37, 0.24],\n",
    "    [0.49, 0.48, 0.44, 0.09, 0.24, 0.2, 0.41, 0.16, 0.1, 0.15],\n",
    "    [0.41, 0.36, 0.27, 0.19, 0.15, 0.42, 0.23, 0.42, 0.02, 0.42],\n",
    "    [0.31, 0.41, 0.21, 0.19, 0.47, 0.28, 0.21, 0.39, 0.16, 0.38],\n",
    "    [0.46, 0.12, 0.21, 0.25, 0.38, 0.38, 0.46, 0.23, 0.31, 0.14],\n",
    "    [0.13, 0.33, 0.28, 0.42, 0.07, 0.13, 0.58, 0.15, 0.0, 0.49],\n",
    "    [0.21, 0.09, 0.07, 0.09, 0.3, 0.54, 0.24, 0.43, 0.51, 0.21],\n",
    "    [0.18, 0.39, 0.42, 0.05, 0.41, 0.1, 0.52, 0.12, 0.14, 0.38],\n",
    "    [0.4, 0.51, 0.01, 0.1, 0.12, 0.22, 0.26, 0.34, 0.42, 0.38]\n",
    "]\n",
    "\n",
    "def cosine(doc1, doc2):\n",
    "    length_doc1 = 0\n",
    "    length_doc2 = 0\n",
    "    scalar = 0.0\n",
    "    if not len(doc1) == len(doc2):\n",
    "        raise ValueError('Vectors of different length')\n",
    "    for i in range(len(doc1)):\n",
    "        scalar += doc1[i] * doc2[i]\n",
    "        length_doc1 += math.pow(doc1[i], 2) \n",
    "        length_doc2 += math.pow(doc2[i], 2)\n",
    "    length_doc1, length_doc2 = math.sqrt(length_doc1), math.sqrt(length_doc2)\n",
    "    return scalar / (length_doc1 + length_doc2)\n",
    "\n",
    "def avg_cluster_similarity(cluster, doc):\n",
    "    similarity = 0.0\n",
    "    for member in cluster:\n",
    "        similarity += cosine(member, doc)\n",
    "    return similarity/len(cluster)\n",
    "    \n",
    "\n",
    "def single_pass_clustering(docs, lamda):\n",
    "    clusters = [[docs[0]]]\n",
    "    for i in range(1, len(docs)):\n",
    "        doc = docs[i]\n",
    "        # this will find the best cluster\n",
    "        max_avg_similarity = 0.0\n",
    "        best_cluster = None\n",
    "        for j in range(len(clusters)):\n",
    "            cluster = clusters[j]\n",
    "            cluster_sim = avg_cluster_similarity(cluster, doc)\n",
    "            if cluster_sim > max_avg_similarity:\n",
    "                best_cluster = j\n",
    "                max_avg_similarity = 0.0\n",
    "        # assign to cluster if greater lamda\n",
    "        if max_avg_similarity > lamda:\n",
    "            clusters[best_cluster].append(doc)\n",
    "        else:\n",
    "            clusters.append([doc])\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def print_clusters(c):\n",
    "    for i in range(len(c)):\n",
    "        print('Cluster %s\\n\\t%s' %(str(i), pprint(c[i])))\n",
    "    \n",
    "spc0_6 = single_pass_clustering(doc_term_idfvector, 15000.4)\n",
    "\n",
    "print_clusters(spc0_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
