{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRWS Homework 03\n",
    "*Sebastian Wagner*\n",
    "## Exercise 1\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "import sklearn.cluster\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining the documents here and set the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOKENIZER = TreebankWordTokenizer()\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "d1 = \"Frodo and Sam were trembling in the darkness, surrounded in darkness by hundreds of blood-thirsty orc. Sam was certain these beasts were about to taste the scent of their flesh.\"\n",
    "d2 = \"The faceless black beast then stabbed Frodo. He felt like every nerve in his body was hurting. Suddenly, he thought of Sam and his calming smile. Frodo had betrayed him.\"\n",
    "d3 = \"Frodo sword was radiating blue, stronger and stronger every second. Orc were getting closer. And these werenâ€™t just regular orc either, Uruk-Hai were among them. Frodo had killed regular orc before, but he had never stabbed an Uruk-Hai, not with the blue stick.\"\n",
    "d4 = \"Sam was carrying a small lamp, shedding some blue light. He was afraid that orc might spot him, but it was the only way to avoid deadly pitfalls of Mordor.\"\n",
    "\n",
    "docs = [d1.lower(), d2.lower(), d3.lower(), d4.lower()]\n",
    "\n",
    "\n",
    "ORDERED_VOC = [t.lower() for t in [\"Frodo\", \"Sam\", \"beast\", \"orc\", \"blue\"]]\n",
    "VOC = set(ORDERED_VOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining some functions here that will be needed later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stolen from http://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def reduce_to_voc(tokens):\n",
    "    t_rel = []\n",
    "    for t in tokens:\n",
    "        if t in VOC:\n",
    "            t_rel.append(t)\n",
    "    return t_rel\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    for t in tagged:\n",
    "        try:\n",
    "            yield LEMMATIZER.lemmatize(t[0], get_wordnet_pos(t[1]))\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "def tokenize(doc):\n",
    "    doc = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in doc]).split())\n",
    "    result = []\n",
    "    for token in TOKENIZER.tokenize(doc):\n",
    "        result.append(token.lower())\n",
    "    return result\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = tokenize(doc)\n",
    "    tokens = lemmatize(tokens)\n",
    "    tokens = reduce_to_voc(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do preprocessing, the follwoing tokens remain\n",
    "documents are numbered horizontally, commencing from doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['frodo', 'sam', 'orc', 'sam', 'beast'],\n",
      " ['beast', 'frodo', 'sam', 'frodo'],\n",
      " ['frodo', 'blue', 'orc', 'orc', 'frodo', 'orc', 'blue'],\n",
      " ['sam', 'blue', 'orc']]\n"
     ]
    }
   ],
   "source": [
    "docs_prep = []\n",
    "for doc in docs:\n",
    "    docs_prep.append(preprocess(doc))\n",
    "pprint(docs_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frodo', 'sam', 'beast', 'orc', 'blue']\n",
      "array([[ 1.33333333,  0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  1.33333333,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  2.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  1.33333333,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ,  2.        ]])\n"
     ]
    }
   ],
   "source": [
    "def compute_idf(docs):\n",
    "    mat = np.zeros((len(ORDERED_VOC), len(ORDERED_VOC)))\n",
    "    no_docs = len(docs)\n",
    "    for doc in docs:\n",
    "        for i in range(len(ORDERED_VOC)):\n",
    "            term = ORDERED_VOC[i]\n",
    "            mat[i,i] += 1 if term in doc else 0\n",
    "    for i in range(len(ORDERED_VOC)):\n",
    "        mat[i,i] = no_docs/(mat[i,i]) if mat[i,i] !=0 else 0\n",
    "    return mat\n",
    "\n",
    "idf = compute_idf(docs_prep)\n",
    "pprint(ORDERED_VOC)\n",
    "pprint(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the tf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 1.,  2.,  1.,  1.,  0.],\n",
      "       [ 2.,  1.,  1.,  0.,  0.],\n",
      "       [ 2.,  0.,  0.,  3.,  2.],\n",
      "       [ 0.,  1.,  0.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "def compute_tf(docs):\n",
    "    mat = np.zeros((len(docs) ,len(ORDERED_VOC)))\n",
    "    for d in range(len(docs)):\n",
    "        doc = docs[d]\n",
    "        for t in range(len(ORDERED_VOC)):\n",
    "            term = ORDERED_VOC[t]\n",
    "            mat[d][t] = doc.count(term)\n",
    "    return mat\n",
    "\n",
    "tf = compute_tf(docs_prep)\n",
    "pprint(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the tf-idf matrix, then transpose it so it is in document-term format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix in document term format\n",
      "array([[ 1.33333333,  2.66666667,  2.        ,  1.33333333,  0.        ],\n",
      "       [ 2.66666667,  1.33333333,  2.        ,  0.        ,  0.        ],\n",
      "       [ 2.66666667,  0.        ,  0.        ,  4.        ,  4.        ],\n",
      "       [ 0.        ,  1.33333333,  0.        ,  1.33333333,  2.        ]])\n"
     ]
    }
   ],
   "source": [
    "tf_idf = np.matmul(tf, idf)\n",
    "term_doc_tfidf = np.transpose(tf_idf)\n",
    "print('TF-IDF matrix in document term format')\n",
    "pprint(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -4.85279372e-01,  -3.20298863e-01,  -7.05471070e-01,\n",
      "          9.51352972e-02,  -3.93919299e-01],\n",
      "       [ -2.41792427e-01,  -5.50491549e-01,   6.63994195e-01,\n",
      "          2.05997018e-01,  -3.93919299e-01],\n",
      "       [ -1.75793034e-01,  -5.83892823e-01,  -6.20129059e-02,\n",
      "         -6.02554933e-02,   7.87838597e-01],\n",
      "       [ -5.98256394e-01,   2.34463181e-01,   2.05534526e-01,\n",
      "         -7.38154362e-01,   5.96744876e-16],\n",
      "       [ -5.63228595e-01,   4.45492851e-01,   1.23823405e-01,\n",
      "          6.32464953e-01,   2.62612866e-01]]),\n",
      " array([[ 7.08256944,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  4.34281253,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  2.12423778,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.82886997]]),\n",
      " array([[-0.34466026, -0.27787293, -0.83868126, -0.31719029],\n",
      "       [-0.6332787 , -0.63458983,  0.42960496,  0.10813604],\n",
      "       [ 0.46136234, -0.52722742, -0.26542436,  0.66236391],\n",
      "       [-0.51702242,  0.49205043, -0.20396868,  0.67005296]])]\n"
     ]
    }
   ],
   "source": [
    "def svd(mat):\n",
    "    return np.linalg.svd(mat)\n",
    "    \n",
    "u, sigma, v = svd(term_doc_tfidf)\n",
    "sigma = np.diag(sigma)\n",
    "    \n",
    "pprint([u, sigma, v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent matrices with K2\n",
      "[array([[-0.48527937, -0.32029886],\n",
      "       [-0.24179243, -0.55049155],\n",
      "       [-0.17579303, -0.58389282],\n",
      "       [-0.59825639,  0.23446318],\n",
      "       [-0.56322859,  0.44549285]]),\n",
      " array([[ 7.08256944,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  4.34281253,  0.        ,  0.        ]]),\n",
      " array([[-0.34466026, -0.27787293],\n",
      "       [-0.6332787 , -0.63458983],\n",
      "       [ 0.46136234, -0.52722742],\n",
      "       [-0.51702242,  0.49205043]])]\n",
      "\n",
      "doc vector in 2 dimensional space\n",
      "array([[-2.44108023, -4.48524039,  3.26763082, -3.66184718],\n",
      "       [-1.20675003, -2.75590468, -2.28964985,  2.13688279]])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f681e5d4cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigma_2 = sigma[:2,:]\n",
    "u_2 = u[:,:2]\n",
    "v_2 = v[:,:2]\n",
    "print('Latent matrices with K2')\n",
    "pprint([u_2, sigma_2, v_2])\n",
    "\n",
    "dense_vec = np.matmul(sigma_2, np.transpose(v))\n",
    "\n",
    "print('\\ndoc vector in 2 dimensional space')\n",
    "pprint(dense_vec)\n",
    "\n",
    "x = dense_vec[:1,:]\n",
    "y = dense_vec[1:2,:]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vector query\n",
      "[array([[ 0.        ],\n",
      "       [ 1.33333333],\n",
      "       [ 0.        ],\n",
      "       [ 1.33333333],\n",
      "       [ 2.        ]])]\n",
      "\n",
      "Query Vector in latent space \n",
      "array([[-2.24652228],\n",
      "       [ 0.46961454]])\n",
      "\n",
      "ranked documents:\n",
      "[{'doc': '4', 'similarity': 0.94855206456837704},\n",
      " {'doc': '1', 'similarity': 0.7867987390042479},\n",
      " {'doc': '2', 'similarity': 0.72687085886729397},\n",
      " {'doc': '3', 'similarity': -0.91905247180497585}]\n"
     ]
    }
   ],
   "source": [
    "query= ['sam', 'blue', 'orc']\n",
    "tf_query = compute_tf([query])\n",
    "tf_idf_query = np.transpose(np.matmul(tf_query, idf))\n",
    "print('TF-IDF vector query')\n",
    "pprint([tf_idf_query])\n",
    "\n",
    "print('\\nQuery Vector in latent space ')\n",
    "dense_query = np.matmul(np.transpose(u_2), tf_idf_query)\n",
    "pprint(dense_query)\n",
    "\n",
    "def cosine(doc1, doc2):\n",
    "    length_doc1 = 0\n",
    "    length_doc2 = 0\n",
    "    scalar = 0.0\n",
    "    if not len(doc1) == len(doc2):\n",
    "        raise ValueError('Vectors of different length')\n",
    "    for i in range(len(doc1)):\n",
    "        scalar += doc1[i] * doc2[i]\n",
    "        length_doc1 += math.pow(doc1[i], 2) \n",
    "        length_doc2 += math.pow(doc2[i], 2)\n",
    "    length_doc1, length_doc2 = math.sqrt(length_doc1), math.sqrt(length_doc2)\n",
    "    return scalar / (length_doc1 * length_doc2)\n",
    "\n",
    "def rank(doc_term, query_term):\n",
    "    ranking = []\n",
    "    i = 0\n",
    "    for doc in (doc_term):\n",
    "        sim = {'doc':  str(i+1),\n",
    "               'similarity' : cosine(doc, query_term)\n",
    "              }\n",
    "        i += 1\n",
    "        ranking.append(sim)\n",
    "    return sorted(ranking, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "\n",
    "    \n",
    "ranked = rank(np.transpose(dense_vec), np.transpose(dense_query)[0])\n",
    "print('\\nranked documents:')\n",
    "pprint(ranked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single pass clustering with lamda=0.6\n",
      "Cluster 0:\n",
      "\t[0.4, 0.51, 0.01, 0.1, 0.12, 0.22, 0.26, 0.34, 0.42, 0.38]; [0.17, 0.21, 0.35, 0.44, 0.49, 0.39, 0.09, 0.07, 0.37, 0.24]; [0.49, 0.48, 0.44, 0.09, 0.24, 0.2, 0.41, 0.16, 0.1, 0.15]; [0.41, 0.36, 0.27, 0.19, 0.15, 0.42, 0.23, 0.42, 0.02, 0.42]; [0.31, 0.41, 0.21, 0.19, 0.47, 0.28, 0.21, 0.39, 0.16, 0.38]; [0.46, 0.12, 0.21, 0.25, 0.38, 0.38, 0.46, 0.23, 0.31, 0.14]; [0.13, 0.33, 0.28, 0.42, 0.07, 0.13, 0.58, 0.15, 0.0, 0.49]; [0.21, 0.09, 0.07, 0.09, 0.3, 0.54, 0.24, 0.43, 0.51, 0.21]; [0.18, 0.39, 0.42, 0.05, 0.41, 0.1, 0.52, 0.12, 0.14, 0.38]\n",
      "Single pass clustering with lamda=0.8\n",
      "Cluster 0:\n",
      "\t[0.18, 0.39, 0.42, 0.05, 0.41, 0.1, 0.52, 0.12, 0.14, 0.38]; [0.49, 0.48, 0.44, 0.09, 0.24, 0.2, 0.41, 0.16, 0.1, 0.15]; [0.41, 0.36, 0.27, 0.19, 0.15, 0.42, 0.23, 0.42, 0.02, 0.42]; [0.31, 0.41, 0.21, 0.19, 0.47, 0.28, 0.21, 0.39, 0.16, 0.38]; [0.46, 0.12, 0.21, 0.25, 0.38, 0.38, 0.46, 0.23, 0.31, 0.14]; [0.13, 0.33, 0.28, 0.42, 0.07, 0.13, 0.58, 0.15, 0.0, 0.49]; [0.21, 0.09, 0.07, 0.09, 0.3, 0.54, 0.24, 0.43, 0.51, 0.21]\n",
      "Cluster 1:\n",
      "\t[0.17, 0.21, 0.35, 0.44, 0.49, 0.39, 0.09, 0.07, 0.37, 0.24]\n"
     ]
    }
   ],
   "source": [
    "doc_term_idfvector = [\n",
    "    [0.17, 0.21, 0.35, 0.44, 0.49, 0.39, 0.09, 0.07, 0.37, 0.24],\n",
    "    [0.49, 0.48, 0.44, 0.09, 0.24, 0.2, 0.41, 0.16, 0.1, 0.15],\n",
    "    [0.41, 0.36, 0.27, 0.19, 0.15, 0.42, 0.23, 0.42, 0.02, 0.42],\n",
    "    [0.31, 0.41, 0.21, 0.19, 0.47, 0.28, 0.21, 0.39, 0.16, 0.38],\n",
    "    [0.46, 0.12, 0.21, 0.25, 0.38, 0.38, 0.46, 0.23, 0.31, 0.14],\n",
    "    [0.13, 0.33, 0.28, 0.42, 0.07, 0.13, 0.58, 0.15, 0.0, 0.49],\n",
    "    [0.21, 0.09, 0.07, 0.09, 0.3, 0.54, 0.24, 0.43, 0.51, 0.21],\n",
    "    [0.18, 0.39, 0.42, 0.05, 0.41, 0.1, 0.52, 0.12, 0.14, 0.38],\n",
    "    [0.4, 0.51, 0.01, 0.1, 0.12, 0.22, 0.26, 0.34, 0.42, 0.38]\n",
    "]\n",
    "\n",
    "\n",
    "def avg_cluster_similarity(cluster, doc):\n",
    "    similarity = 0.0\n",
    "    for member in cluster:\n",
    "        similarity += cosine(member, doc)\n",
    "    sim = similarity/len(cluster)\n",
    "    return sim\n",
    "\n",
    "def max_cluster_similarity(cluster, doc):\n",
    "    max_similarity = 0.0\n",
    "    for member in cluster:\n",
    "        current_sim = cosine(member, doc)\n",
    "        if current_sim > max_similarity:\n",
    "            max_similarity = current_sim\n",
    "    return max_similarity\n",
    "    \n",
    "def single_pass_clustering(docs, lamda, cluster_similarity):\n",
    "    \"\"\"\n",
    "    returns: clusters; first array is the list of clusters, which then contains a list of documents\n",
    "    \"\"\"\n",
    "    clusters = []\n",
    "    clusters.append([docs.pop(-1)])\n",
    "    # loop over each doc and try to assign to cluster\n",
    "    for doc in docs:\n",
    "        max_similarity = 0\n",
    "        best_cluster = None\n",
    "        for cluster in clusters:\n",
    "            # determine the best matching cluster, lamda will be checked later on\n",
    "            sim = cluster_similarity(cluster, doc)\n",
    "            if sim > max_similarity:\n",
    "                max_similarity = sim\n",
    "                best_cluster = cluster\n",
    "        if lamda > max_similarity:\n",
    "            # create a new cluster\n",
    "            clusters.append([doc])\n",
    "        else:\n",
    "            # append to existing cluster\n",
    "            best_cluster.append(doc)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def print_clusters(clusters):\n",
    "    i = 0\n",
    "    for cluster in clusters:\n",
    "        print('Cluster %s:' % (str(i)))\n",
    "        i += 1\n",
    "        print('\\t' + \"; \".join([str(doc) for doc in cluster]))\n",
    "    \n",
    "spc0_6 = single_pass_clustering(doc_term_idfvector, 0.6, max_cluster_similarity)\n",
    "spc0_8 = single_pass_clustering(doc_term_idfvector, 0.8, max_cluster_similarity)\n",
    "\n",
    "print('Single pass clustering with lamda=0.6')\n",
    "print_clusters(spc0_6)\n",
    "\n",
    "print('Single pass clustering with lamda=0.8')\n",
    "print_clusters(spc0_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In general a lower lamda will lead to less clusters as the algorithm will assign a document with lower similarities to an existing cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single pass clustering with lamda=0.8 and reversed streamlining\n",
      "Cluster 0:\n",
      "\t[0.17, 0.21, 0.35, 0.44, 0.49, 0.39, 0.09, 0.07, 0.37, 0.24]\n",
      "Cluster 1:\n",
      "\t[0.21, 0.09, 0.07, 0.09, 0.3, 0.54, 0.24, 0.43, 0.51, 0.21]; [0.46, 0.12, 0.21, 0.25, 0.38, 0.38, 0.46, 0.23, 0.31, 0.14]; [0.31, 0.41, 0.21, 0.19, 0.47, 0.28, 0.21, 0.39, 0.16, 0.38]; [0.41, 0.36, 0.27, 0.19, 0.15, 0.42, 0.23, 0.42, 0.02, 0.42]; [0.49, 0.48, 0.44, 0.09, 0.24, 0.2, 0.41, 0.16, 0.1, 0.15]\n",
      "Cluster 2:\n",
      "\t[0.13, 0.33, 0.28, 0.42, 0.07, 0.13, 0.58, 0.15, 0.0, 0.49]\n"
     ]
    }
   ],
   "source": [
    "spc0_8_reversed = single_pass_clustering([d for d in reversed(doc_term_idfvector)], 0.8, max_cluster_similarity)\n",
    "print('Single pass clustering with lamda=0.8 and reversed streamlining')\n",
    "print_clusters(spc0_8_reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters change, which is entailed by the different start cluster the algorithm has chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centers\n",
      "array([[ 0.3725,  0.3225,  0.3   ,  0.2375,  0.21  ,  0.2825,  0.42  ,\n",
      "         0.24  ,  0.1075,  0.3   ],\n",
      "       [ 0.24  ,  0.31  ,  0.28  ,  0.315 ,  0.48  ,  0.335 ,  0.15  ,\n",
      "         0.23  ,  0.265 ,  0.31  ],\n",
      "       [ 0.21  ,  0.09  ,  0.07  ,  0.09  ,  0.3   ,  0.54  ,  0.24  ,\n",
      "         0.43  ,  0.51  ,  0.21  ]])\n",
      "Doc 1 [0.17, 0.21, 0.35, 0.44, 0.49, 0.39, 0.09, 0.07, 0.37, 0.24] assigned to cluster 2\n",
      "Doc 2 [0.49, 0.48, 0.44, 0.09, 0.24, 0.2, 0.41, 0.16, 0.1, 0.15] assigned to cluster 1\n",
      "Doc 3 [0.41, 0.36, 0.27, 0.19, 0.15, 0.42, 0.23, 0.42, 0.02, 0.42] assigned to cluster 1\n",
      "Doc 4 [0.31, 0.41, 0.21, 0.19, 0.47, 0.28, 0.21, 0.39, 0.16, 0.38] assigned to cluster 2\n",
      "Doc 5 [0.46, 0.12, 0.21, 0.25, 0.38, 0.38, 0.46, 0.23, 0.31, 0.14] assigned to cluster 1\n",
      "Doc 6 [0.13, 0.33, 0.28, 0.42, 0.07, 0.13, 0.58, 0.15, 0.0, 0.49] assigned to cluster 1\n",
      "Doc 7 [0.21, 0.09, 0.07, 0.09, 0.3, 0.54, 0.24, 0.43, 0.51, 0.21] assigned to cluster 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cluster/k_means_.py:889: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "centroids = [\n",
    "    [0.33, 0.33, 0.42, 0.12, 0.2, 0.34, 0.58, 0.19, 0.07, 0.24],\n",
    "    [0.29, 0.16, 0.38, 0.48, 0.43, 0.11, 0.12, 0.33, 0.03, 0.44],\n",
    "    [0.01, 0.17, 0.11, 0.27, 0.23, 0.37, 0.35, 0.48, 0.54, 0.24]\n",
    "]\n",
    "centroids_np = np.array(centroids)\n",
    "\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=3, init=centroids_np).fit(np.array(doc_term_idfvector))\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "print('Cluster centers')\n",
    "pprint(centers)\n",
    "\n",
    "i = 0\n",
    "for doc in doc_term_idfvector:\n",
    "    clusters = []\n",
    "    best_cluster = None\n",
    "    max_sim = 0.0\n",
    "    for center in range(len(centers)):\n",
    "        sim = avg_cluster_similarity([centers[center]], doc)\n",
    "        if max_sim < sim:\n",
    "            max_sim = sim\n",
    "            best_cluster = center\n",
    "    print('Doc %s %s assigned to cluster %s' %(str(i+1), doc, str(best_cluster+1))) \n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r1': {'f1': 0.4, 'precision': 0.6, 'recall': 0.3},\n",
      " 'r2': {'f1': 0.6666666666666666, 'precision': 0.5, 'recall': 1.0},\n",
      " 'r3': {'f1': 0.47619047619047616,\n",
      "        'precision': 0.45454545454545453,\n",
      "        'recall': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = set([i for i in range(1,20) if i % 2 !=0])\n",
    "\n",
    "results = {\n",
    "    'r1' : set([1, 2, 5, 6, 13]),\n",
    "    'r2' : set([1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 19, 14, 17, 3, 15, 16, 18, 20]),\n",
    "    'r3' : set([1, 2, 4, 5, 9, 10, 12, 13, 14, 15, 20])\n",
    "}\n",
    "\n",
    "\n",
    "def compute_precision(relevant, retrieved):\n",
    "    tp = len(set.intersection(relevant, retrieved))\n",
    "    return tp/len(retrieved)\n",
    "\n",
    "def compute_recall(relevant, retrieved):\n",
    "    tp = len(set.intersection(relevant, retrieved))\n",
    "    return tp/len(relevant)\n",
    "\n",
    "def compute_fmeasure(f, relevant, retrieved):\n",
    "    precision = compute_precision(relevant, retrieved)\n",
    "    recall = compute_recall(relevant, retrieved)\n",
    "    return 1/((f*(1/precision))+(1-f)*(1/recall))\n",
    "\n",
    "\n",
    "def compute_f1measure(relevant, retrieved):\n",
    "    precision = compute_precision(relevant, retrieved)\n",
    "    recall = compute_recall(relevant, retrieved)\n",
    "    return 2 * ( (precision *recall) / (precision + recall))\n",
    "    \n",
    "for k,v in results.items():\n",
    "    evaluation = {\n",
    "        'precision' : compute_precision(relevant_docs, v),\n",
    "        'recall' : compute_recall(relevant_docs, v),\n",
    "        'f1' : compute_f1measure(relevant_docs, v)\n",
    "    }\n",
    "    results[k] = evaluation\n",
    "pprint(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these measures are only suitable for binary evaluation but cannot evaluate rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r1': {'average precision': 0.7555555555555555,\n",
      "        'p@5': 0.6,\n",
      "        'r-precision': 0.3},\n",
      " 'r2': {'average precision': 0.5722530165912518,\n",
      "        'p@5': 0.4,\n",
      "        'r-precision': 0.5},\n",
      " 'r3': {'average precision': 0.62, 'p@5': 0.6, 'r-precision': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = set([i for i in range(1,20) if i % 2 !=0])\n",
    "results = {\n",
    "    'r1' : [1, 2, 5, 6, 13],\n",
    "    'r2' : [1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 19, 14, 17, 3, 15, 16, 18, 20],\n",
    "    'r3' : [1, 2, 4, 5, 9, 10, 12, 13, 14, 15, 20]\n",
    "}\n",
    "\n",
    "\n",
    "def average_precision(relevant_docs, ranking):\n",
    "    ranks = []\n",
    "    for item in relevant_docs:\n",
    "        try:\n",
    "            ranks.append(compute_precision(relevant_docs, ranking[:ranking.index(item)+1]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return sum([add for add in ranks]) / len(ranks)\n",
    "\n",
    "def compute_rprecision(relevant, retrieved):\n",
    "    tp = len(set.intersection(relevant, retrieved))\n",
    "    return tp/len(relevant)\n",
    "    \n",
    "\n",
    "for k,v in results.items():\n",
    "    evaluation = {\n",
    "        'p@5' : compute_precision(relevant_docs, set(v[:5])),\n",
    "        'r-precision' : compute_rprecision(relevant_docs, set(v[:len(relevant_docs)])),\n",
    "        'average precision' : average_precision(relevant_docs, v)\n",
    "    }\n",
    "    results[k] = evaluation\n",
    "\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision is \n",
      " 0.5521577964519141\n"
     ]
    }
   ],
   "source": [
    "q1 = [1, 6, 9, 17, 21]\n",
    "q2 = [1, 3, 4]\n",
    "q3 = [2, 5, 8, 9, 10]\n",
    "q4 = [4]\n",
    "q5 = [1, 2, 6]\n",
    "\n",
    "queries = [q1, q2, q3, q4, q5]\n",
    "\n",
    "def precision(no_rel, rank):\n",
    "    return no_rel/rank\n",
    "    \n",
    "\n",
    "def average_precision(ranks):\n",
    "    return sum([precision(ranks.index(r)+1,  r) for r in ranks])/len(ranks)\n",
    "\n",
    "def mean_average_precision(queries):\n",
    "    return sum([average_precision(query) for query in queries])/len(queries)\n",
    "\n",
    "map_ = mean_average_precision(queries)\n",
    "print('Mean average precision is \\n %s' % map_) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
